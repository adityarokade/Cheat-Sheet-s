{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Find relation between input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Comman Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)Over Fitting:-\n",
    "low bias but high variance                      \n",
    " The situation where any given model is performing to well on training data but performance drops significantly on test data or unknown data.\n",
    "\n",
    "#### 2) Under Fitting:-\n",
    "The model performing poor on train as well as test data,it is called as Underfitting condition.\n",
    "\n",
    "#### 3) Data Lickage:-\n",
    "In Ml we pass some data to test and some to train,if data is comman in both then our test data is not unbiased this is called data lickage.\n",
    "\n",
    "#### 4) Data Wrangling:- \n",
    "It involves the reorganizing,summarizing,mapping,transforming,etc from its raw.                   \n",
    "it convert the unstructed data to some useful format\n",
    "\n",
    "#### 5) Data Argumentation:-\n",
    "Generating the some synthesised(new data by using old data) data.\n",
    "\n",
    "#### 6) Data Drifting:-\n",
    "Data drift occurs when model get trained on changes.\n",
    "The change in input data or independent variable leads to poor performance of the model\n",
    "\n",
    "#### 7)Bias:-\n",
    "Bias is considered a systematic error that occurs in the machine learning model itself due to incorrect assumptions in the ML process\n",
    "\n",
    "#### 8) Variance:-\n",
    "variance tells that how much a random variable is different from its expected value.                          \n",
    "value which tells us spread of our data. \n",
    "\n",
    "#### 9) Bias and Variance trade off:-\n",
    "Balance between both to avoid the overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent :-\n",
    "It is used to minimize the function by optimizing the it's parameter.\n",
    "###### new_value=old_value-step_size\n",
    "\n",
    "stepsize=learning rate * slope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Provide input and output data\n",
    "\n",
    "##### 1) Regression:- \n",
    "Concept of Prediction a Contineous Value\n",
    "##### 2) classification:-\n",
    "It is a process of categorizing a given set of data into classes.it can be performed on stryctured or unstructed data.\n",
    "\n",
    "It is used to predict the class of given new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression algorithms:-\n",
    "1)Linear Regression\n",
    "\n",
    "\n",
    "##### Classification algorithms\n",
    "1) Logistic Regression        \n",
    "2) Naive Bayes                              \n",
    "\n",
    "##### Regression & Classification algorithms\n",
    "1) Decision Tree                 \n",
    "2) Random Forest                 \n",
    "3) KNN                                        \n",
    "4) SVM                                 \n",
    "     \n",
    "\n",
    "##### Ensemble Technique\n",
    " Boosting,Bagging,Stacking\n",
    " Ada boast , Gradient Boasted , XG-boast    \n",
    "##### Clustering \n",
    "K-Means,Herichical,DBSCAN\n",
    "\n",
    "##### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model  Evolution\n",
    "\n",
    "### $R^{2}$:-\n",
    "It is statical term which is used to measure the goodness of regression model(accuracy).           \n",
    "$R^{2}$=1-{variance(line) / variance(mean)}                \n",
    "\n",
    "$R^{2}$= 1-{ sum of resuadial / sum of avg total }         \n",
    " \n",
    "\n",
    "### Adjusted $R^{2} $ :- \n",
    " When we adding a new independent variable then our $R^{2}$ is incesses due to we use adjusted $R^{2}$ .           \n",
    " \n",
    " Adjusted- $R^{2}$ =1-{[(1-R^2)(N-1)] / N-P-1}                 \n",
    " \n",
    " When the indepedent feature have a corelation with target feature then the value of adjustent $R^2$ is increses and when there is no corelation between feature and target then the value get decreases.\n",
    " \n",
    " \n",
    "## Cassification Model Evolution\n",
    " 1) confusion matrix\n",
    "             \n",
    "             \n",
    "                     Acual \n",
    "                 1          0\n",
    "             1   TP          FP\n",
    "             0   FN          TN\n",
    "  \n",
    "  \n",
    "  \n",
    " 2) Accuracy\n",
    "    \n",
    "    accuracy = {TP + TN } / { TP + FP + FN + TN}\n",
    "    \n",
    " 3) Recall ( Sensitivity ) How sensitive our model to take only possitive models.\n",
    " \n",
    "    = { TP } / { TP + FN }\n",
    "    \n",
    " 4) Precision - in possitive prediction how much are realy positive\n",
    " \n",
    "    = { TP } / { TP + FP }\n",
    " \n",
    " 5) F1 score\n",
    " \n",
    "     ={2( P * R) / (P + R) }\n",
    " \n",
    " 6) Specivity                      \n",
    " 7) AUC-area under curve     out of multiple column to select the perticular one auc is used .               \n",
    " 8) ROC-Reciver operator characteristic    Mostely used to choose the best model or thereshold.               \n",
    " \n",
    "The curve TPR(y) VS FPR(x)          >>>>       TPR- true possitive rate, FPR- false possitive rate. \n",
    "\n",
    "### VIF -variance Influence factor:-\n",
    " Used to calculate the multi-collinearity                              \n",
    " if value is VIF>10 then high co-relate.                                         \n",
    " VIF = (1/(1-$r^{2}$)                        \n",
    " \n",
    "\n",
    "### Cross Validation :-\n",
    "Resampling technique.            \n",
    "Divide the data into two parts --> train,test.          \n",
    " 1) K-Fold cross validation :- devide the data into k subsets and then 1st set is used to test and others are used to train . Error is calculated and process is contineu to K times.\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "High bias and low variance\n",
    "\n",
    "###### y=mx+c\n",
    "\n",
    "#### Assiamptions:-\n",
    "1) (X) R (Y)                     \n",
    "2) Mean of Resudial should be zero                      \n",
    "3) Error are not suppose to corelated        \n",
    "4) Exogenity :- x & resuduails are uncorelated         \n",
    "5) Homoscesciticity :-Error term must show constant variance                     \n",
    "6) No multi multicollinearity                      \n",
    "7) Error term are suppose to be normally distributed.\n",
    "\n",
    "\n",
    "###### Residual =y- $\\hat{y}$\n",
    "\n",
    "\n",
    "\n",
    "###### MultiRegression :-\n",
    "y=$M_{1}X_{1}$ + $M_{2}X_{2}$ + $M_{3}X_{3}$ +.............+  $M_{n}X_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization :-\n",
    "Used to avoid the overfitting condition.                                             \n",
    "To control the Error term .      \n",
    "\n",
    "Try to Change the changes forcely based on prevised changes.\n",
    "\n",
    "1)LASSO :- Least absolute shrinkage & selection operator\n",
    "used to predict the more accurate result.\n",
    "\n",
    "2) Ridge Regression\n",
    "\n",
    "\n",
    "3) ElasticNet                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Classification\n",
    "\n",
    "\n",
    "accept the outcomes based on predefine classes-classification                   \n",
    "Draw a line to Seperate a Classes.                           \n",
    "LOSS Fun = log( P(x) / 1-P(x) )=  $\\Theta$  = $B_{1}X + B_{0}$\n",
    "\n",
    "\n",
    "In Logistic regression the regulation is used by default.              \n",
    "Solvers-      Algorithm to use in optimization.      \n",
    "\n",
    "Newton-cg,saga,lbfgs ---> L2 regulization            \n",
    "liblinear ---> L1 & L2 regulization                     \n",
    "saga ---> ElasticNet              \n",
    "\n",
    "liblinear-Binary class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "It is Regression and Calssification Algorithm                          \n",
    "First Seperate the data and at the end found regression/classification\n",
    "\n",
    "1) Gini-Impurity (only applicable on categorical data)\n",
    "   Select the Column which has less impurity(Gini) to Root Node. \n",
    "   \n",
    "   Gini= 1-$\\sum_{i=1}^{c} (P_{i})^{2}$                               \n",
    "   Gini(column)={ no of instance of class / total no of instance } * G(respected_class)\n",
    "   \n",
    "2) Entropy-Imformation Gain\n",
    "    \n",
    "    Entropy-randomness,degree of freedom.\n",
    "    Low entropy good for model\n",
    "    \n",
    "    Entropy= -p log (p)\n",
    "    Informationgain=diff between entropy after creating tree and before creating tree.\n",
    "    I.G= $ E_{beforeT} - E _{afterT}$\n",
    "    \n",
    "    \n",
    "    \n",
    "######    Different Class:-\n",
    "     Feature                        Outcome-\n",
    "     \n",
    "     Categorical                 Categorical                   Classification\n",
    "     \n",
    "     Contineous                  Categorical                    Classification\n",
    "     \n",
    "     Contineous                  Contineous                     Regression\n",
    "     \n",
    "    \n",
    "###### Different types of algo in DT to solve dffeternt problems\n",
    "\n",
    "1) id3 ---> iterative dectomizer => classification              \n",
    "2) C4.5 ---> revision of id3  => classification                 \n",
    "3) CART ---> Classification and regression tree.                         \n",
    "             devide the dataset based on thereshold,use multile thereshold                            \n",
    "             and $\\hat{y}$ calculated based on average\n",
    "             \n",
    "             \n",
    "             \n",
    "##### Pruning\n",
    "1) Post Pruning | Backword Pruning                            \n",
    "    1st generate the complete tree and then identify the which branch is not participated or by which branch it gives overfitting condition.                     \n",
    "    It will identified by using cross validation\n",
    "    \n",
    "    cross validation:- It tune the variable and which branch is not participated it will remove that one.\n",
    "    \n",
    "2) Pre Pruning  | Forword Pruning                                    \n",
    "   Stop a DT to create a non significant branches before build a tree. \n",
    "  \n",
    "  \n",
    "  \n",
    "## Random Forest\n",
    "Low bias and high variance                                     \n",
    " It is kind of bagging ensemble technique algorithm.in which the DT is a base algorithm.we can use n number of DT algorithm as a base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Technique\n",
    "1) Bagging -same nature                                             \n",
    "    BootStrap Aggression | BootStrapping                           \n",
    "Every model takes it's own Decision.                        \n",
    "e.g - Mamebers of parlament                         \n",
    "     Combine multiple Homogeneous decision maker which will take our own decision and final decision will be based on majority.\n",
    "     \n",
    "  BootStrapping:-way to devide the dataset with replacement-- descrise variance without incressing bias.\n",
    "  \n",
    "  Pasting:- Devide the dataset with replacement                      \n",
    "  e.g- train_test_split\n",
    "     \n",
    "\n",
    "2) Boosting - same nature\n",
    "next we are loocking for better Decision.                                 \n",
    "e.g - transition | switch carier                     \n",
    "    i) Ada boost                               \n",
    "    ii) Gradient boosted tree              \n",
    "    iii) XG boost                                \n",
    "3) Stacking-different nature               \n",
    "\n",
    " ```                                            Data(A)\n",
    " \n",
    " \n",
    "                         Train(B)                                                           Val-Train(C)\n",
    "                       \n",
    "                      \n",
    "            \n",
    "               train(D)              test(E)      \n",
    "              \n",
    "              \n",
    "              \n",
    "        KNN(F)         SVM(F)\n",
    "        \n",
    "        \n",
    "        \n",
    "   Prediction        prediction   -{using  Val-Train}\n",
    "   \n",
    "   \n",
    "           Combine ---> (input) Random Forest(val_train) ---> to check score using x_test\n",
    "    \n",
    "   \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "supervised | lazy learning algo.\n",
    "\n",
    "E.D=$\\sqrt {(X_{2} - X_{1})^{2} + (Y_{2} - Y_{1})^{2} }$                \n",
    "\n",
    "select the value of k - odd like 3.                         \n",
    "K- minimum distance     ,It will be calculated.\n",
    "\n",
    "For categorical data --> Hamming Distance formula.\n",
    "\n",
    "KD-Tree , Ball-Tree  - handle the arragement data to avoid computational power\n",
    "\n",
    "######  Kd Tree \n",
    "K-dimensional tree                            \n",
    " Rearrange the data in binary tree so we can easily search the required output- hierachical data structure\n",
    " \n",
    "###### Ball Tree\n",
    " i) Two clusters are created             \n",
    " ii) All data points must be belong to the at least one cluster\n",
    " iii) One point cannot be in both cluster\n",
    " iv) again clusters are devided into subclusters till certain depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM \n",
    "##### SVC | SVR\n",
    "Regression | Classification\n",
    "\n",
    "Draw a Hyper plane line then find the nearest point and draw margin line on both side .the distance between plane and margin will be max.\n",
    "\n",
    "plane--> support vector\n",
    "\n",
    "RBF-Radium Bias Function - used kernal function \n",
    "\n",
    "Try to evaluate dimension of dataset to get better seperation -kernal trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Classification  - Probability based                            \n",
    "If Xi having independently some relation with y then we can use Naive Bayes\n",
    "\n",
    "P(A/B)={ P(B/A) .P(A) } / P(B)\n",
    "\n",
    "eg- May i go to outside for night party.   \n",
    "features- Money,Whether,Friends\n",
    "\n",
    "Can i play a Golf or Not                    \n",
    "features - outlook,temp,windy,humidity,etc\n",
    "\n",
    "Naive bayes work on prehappen events ,if some events is not happen in this case algo is fail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semi-supervised learning\n",
    "combination of supervised and unsupervised                            \n",
    "\n",
    "\n",
    "#### CLustering \n",
    "It groups unlabled data\n",
    "\n",
    "i) K-mean                                 \n",
    "ii) Hierachical clustering                         \n",
    "iii) DBSCAN\n",
    "\n",
    "Usecases- \n",
    "Costomer sengmentation , Data Analysis , Outliers detection , Search engine , Image Segmentation\n",
    "\n",
    "\n",
    "###### Algomerative :-\n",
    "All points are individual cluster & finds similarity between points and make a cluster until there is only one cluster                                            \n",
    "Bottum up approch\n",
    "\n",
    "###### Divisive :-\n",
    "All points are part of big cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mean\n",
    "###### Centroid based clustering\n",
    "\n",
    "Find the value of k by using WCSS apporach                                                \n",
    "WCSS- within controid summation square                     \n",
    "\n",
    "Devide the data into n groups and select the k random points and start grouping and due to that the centroid is shifted.                          \n",
    "calculate the distance of points to centroid using Equalibrim destance formula.\n",
    "\n",
    "\n",
    "Inertia:- WCSS\n",
    "\n",
    "if K = n  then wcss is 0 (zero)\n",
    "if K = 1  then distance is max,wcss is max \n",
    "\n",
    "Elbow Method\n",
    "it is graph of wcss(y) vs k(x)              \n",
    "find the dispersion point -from where there is no major change is happen -dispersion point .               \n",
    "based on that find the value of k\n",
    "\n",
    "\n",
    "Drawback of K-mean               \n",
    "allocation of centroid         \n",
    "\n",
    "###### K-mean++\n",
    "it always select the points with some certain distance of centroid due to that colabration                                    \n",
    "Remove drawback of k-mean   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## herchical Clustering\n",
    "Algomerative apporach                 \n",
    "bootom -up apporach                       \n",
    "start with n no of data points means n cluster                \n",
    "consider the n no of cluster ,and devide based on some similarity\n",
    "\n",
    "It is shown by using $ Dendogram $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "- Density based spacial clustering of application with noise\n",
    "\n",
    "Epsilon ---> Radius                                                                     \n",
    "min-points --->   Minimum points in cluster                         \n",
    "Core-points --->  points in cluster                                \n",
    "Border-points --->  points are parts of  cluster but they not form a new cluster                        \n",
    "Noise ---> Not a part of any cluster                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vlidation | Cluster Evaluation\n",
    "        ```\n",
    "        \n",
    "                                Prediction From clustering \n",
    "                                        \n",
    "                                   1 s                        0 d\n",
    "                                   \n",
    "                          1s        SS                         SD\n",
    "                             \n",
    "    Ground truth ```                     \n",
    "                             \n",
    "                          od        DS                         DD\n",
    "                                        \n",
    "                                 \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "###### 1) Rand index\n",
    "   = { SS + DD } / { SS + DD + SD + DS }\n",
    "  \n",
    "###### 2) Jaccord coefficient\n",
    "        Jc= { SS } / { SS + SD + DS }\n",
    "        \n",
    "###### 3) Entropy \n",
    "  = - $\\sum{P}_{i}log P_{i} $\n",
    "  calculate the randomness or scatterness\n",
    " \n",
    "###### 4) Silhouete coefficient \n",
    "S(x)= { b(x) - a(x) } / max{a(a)or b(x)}                  \n",
    "a(x)--> avg distance of x from all the others points in the same cluster\n",
    "b(x)--> avg distance of x from all the others points in the other cluster\n",
    "###### 5) Purity\n",
    "Total % of data points we are able to place correctly inside the groups | cluster\n",
    "\n",
    "P(c) = { $\\sum M_{i} P_{i} $ } / { n } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
